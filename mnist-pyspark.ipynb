{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST K-means Clustering using PySpark on SageMaker \n",
    "\n",
    "This notebook is based on the example notebook provided by Amazon SageMaker \n",
    "\n",
    "https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-spark/pyspark_mnist/pyspark_mnist_kmeans.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Training Models\n",
    "\n",
    "### Step 1\n",
    "import modules and create ```SparkSession``` with required dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-76-83.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f76b27d1fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get my execution role as defined based on my IAM policy\n",
    "role = get_execution_role()\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "jars = sagemaker_pyspark.classpath_jars()\n",
    "\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.extraClassPath\", classpath).master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# start SparkSession\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Setup instance regions and initialize endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set region\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# set endpoint\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.{}.amazonaws.com'.format(region))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Load Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|(784,[152,153,154...|\n",
      "|  0.0|(784,[127,128,129...|\n",
      "|  4.0|(784,[160,161,162...|\n",
      "|  1.0|(784,[158,159,160...|\n",
      "|  9.0|(784,[208,209,210...|\n",
      "|  2.0|(784,[155,156,157...|\n",
      "|  1.0|(784,[124,125,126...|\n",
      "|  3.0|(784,[151,152,153...|\n",
      "|  1.0|(784,[152,153,154...|\n",
      "|  4.0|(784,[134,135,161...|\n",
      "|  3.0|(784,[123,124,125...|\n",
      "|  5.0|(784,[216,217,218...|\n",
      "|  3.0|(784,[143,144,145...|\n",
      "|  6.0|(784,[72,73,74,99...|\n",
      "|  1.0|(784,[151,152,153...|\n",
      "|  7.0|(784,[211,212,213...|\n",
      "|  2.0|(784,[151,152,153...|\n",
      "|  8.0|(784,[159,160,161...|\n",
      "|  6.0|(784,[100,101,102...|\n",
      "|  9.0|(784,[209,210,211...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData = spark.read.format('libsvm').option('numFeatures', '784').load('s3a://sagemaker-sample-data-{}/spark/mnist/train/'.format(region))\n",
    "\n",
    "testData = spark.read.format('libsvm').option('numFeatures', '784').load('s3a://sagemaker-sample-data-{}/spark/mnist/test/'.format(region))\n",
    "\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Create K-Means Estimator, configured based on IAMRole and specify types of instances to be used for training and model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_pyspark import IAMRole\n",
    "from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator\n",
    "from sagemaker_pyspark import RandomNamePolicyFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_estimator = KMeansSageMakerEstimator(\n",
    "    sagemakerRole = IAMRole(role),\n",
    "    trainingInstanceType = 'ml.m4.xlarge', # Instance type to train K-means on SageMaker\n",
    "    trainingInstanceCount = 1,\n",
    "    endpointInstanceType = 'ml.t2.large', # Instance type to serve model (endpoint) for inference\n",
    "    endpointInitialInstanceCount = 1,\n",
    "    namePolicyFactory = RandomNamePolicyFactory(\"sparksm-1a-\")) # All the resources created are prefixed with sparksm-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Set parameters for k-means and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for K-Means\n",
    "kmeans_estimator.setFeatureDim(784)\n",
    "kmeans_estimator.setK(10)\n",
    "\n",
    "# fit model\n",
    "initialModel = kmeans_estimator.fit(trainingData)\n",
    "\n",
    "# get initial model endpoint name \n",
    "initialModelEndpointName = initialModel.endpointName\n",
    "print(initialModelEndpointName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Run inferences on the test data using the fitted model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedData = initialModel.transform(testData)\n",
    "\n",
    "# show results\n",
    "transformedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
